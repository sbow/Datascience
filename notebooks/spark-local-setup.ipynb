{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark-local-setup.ipynb\n",
    "- Purpose:\n",
    "  - Local development\n",
    "    - Dont runup cloud compute fees to debug\n",
    "    - Better debugger\n",
    "    - Better editor\n",
    "    - No missuse of company property\n",
    "- What it is:\n",
    "  - Demo workbook describing setup & basic use of spark / pyspark\n",
    "- What are the dependancies:\n",
    "  - pipy package \"localstack\", \n",
    "    - installed w pipenv, in Pipfile dev-package\n",
    "    - pipenv install --dev locakstack\n",
    "  - docker.io, docker-compose required for localstack\n",
    "    - this is just the docker runtime\n",
    "  - docker compose file for setup\n",
    "    - docker-compose.yml\n",
    "    - this is the config for localstack / local s3\n",
    "    - ths is the config for spark-py, apache spark container\n",
    "- Startup\n",
    "  - (in bash / zsh)\n",
    "  - > pipenv shell \n",
    "    - start virtualenv\n",
    "  - > docker-compose up -d\n",
    "    - starts LocalStack\n",
    "  - run notebook\n",
    "  - > docker-compose down\n",
    "    - stops LocalStack\n",
    "- Hardware tested:\n",
    "  - Linux (Mint, on metal)\n",
    "    - ❯ lsb_release -d\n",
    "    - Description:\tLinux Mint 21.2\n",
    "    -  inxi -Fxz\n",
    "    - Kernel: 5.15.0-79-generic x86_64\n",
    "- Localstack container filesystem details\n",
    "```\n",
    "/\n",
    "├── etc\n",
    "│   └── localstack\n",
    "│       └── init\n",
    "├── usr\n",
    "│   └── lib\n",
    "│       └── localstack\n",
    "└── var\n",
    "    └── lib\n",
    "        └── localstack  <- the LocalStack volume directory root\n",
    "            ├── cache\n",
    "            ├── lib\n",
    "            ├── logs\n",
    "            ├── state\n",
    "            └── tmp\n",
    "```\n",
    "- Localstack debuging:\n",
    "  - localstack status (from inside pipenv, ie after pipenv shell )\n",
    "  - localstack logs\n",
    "  - docker-compose logs localstack\n",
    "\n",
    "- Apache spark container\n",
    "  - defined in docker-compose.yml\n",
    "  - https://hub.docker.com/r/apache/spark-py\n",
    "  - must manually sudo chmod 777 the local \"spark-...\" directories\n",
    "\n",
    "- Debugging apache spark container\n",
    "  - first run w docker-compose up -d\n",
    "  - enter the docker container\n",
    "    - docker exec -it spark_container /bin/bash\n",
    "    - cd ~/bin\n",
    "    - ./pyspark\n",
    "    - >>> rdd = sc.parallelize([1,2,3,4,5])\n",
    "    - >>> squared = rdd.map(lambda x: x * x).collect()\n",
    "    - >>> print(squared)\n",
    "    - >>> exit()\n",
    "    - exit\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark://192.168.0.2:7077\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only remote Spark sessions using Databricks Connect are supported. Could not find connection parameters to start a Spark remote session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(spark_master_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mRemoteSpark\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m.\u001b[39;49mmaster(spark_master_url) \\\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/shaun/git/github/sbow/Datascience/notebooks/spark-local-setup.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Datascience-6wAOvilU/lib/python3.10/site-packages/pyspark/sql/session.py:488\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot start a remote Spark session because there \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mis a regular Spark session already running.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mOnly remote Spark sessions using Databricks Connect are supported. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    490\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mCould not find connection parameters to start a Spark remote session.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m session \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39m_instantiatedSession\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m session \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m session\u001b[39m.\u001b[39m_sc\u001b[39m.\u001b[39m_jsc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only remote Spark sessions using Databricks Connect are supported. Could not find connection parameters to start a Spark remote session."
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = \"../datascience_shell.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "spark_master_url = os.getenv(\"SPARK_MASTER_URL\")\n",
    "\n",
    "print(spark_master_url)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RemoteSpark\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Datascience-6wAOvilU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
