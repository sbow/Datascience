version: "3.8"

services:
  localstack:
    container_name: "${LOCALSTACK_DOCKER_NAME-localstack_main}"
    image: localstack/localstack
    ports:
      - "127.0.0.1:4566:4566"            # LocalStack Gateway
      - "127.0.0.1:4510-4559:4510-4559"  # external services port range
    environment:
      - SERVICES=S3
      - DEBUG=${DEBUG-}
      - DOCKER_HOST=unix:///var/run/docker.sock
      - LOCALSTACK_PERSISTENCE=1
    volumes:
      - "${LOCALSTACK_VOLUME_DIR:-./volume}:/var/lib/localstack"
      - "/var/run/docker.sock:/var/run/docker.sock"

  spark:
    image: apache/spark-py:latest
    container_name: spark_container
    ports:
      - "4040:4040"  # Spark UI
    command: /bin/bash -c "/opt/spark/sbin/start-master.sh && /opt/spark/sbin/start-worker.sh spark://localhost:7077 && tail -f /dev/null"
    environment:
      - PYSPARK_PYTHON=python3
      - SPARK_WORKER_DIR=/spark-worker
      - SPARK_LOCAL_IP=0.0.0.0
    volumes:
      - ./spark-data:/spark-data  # Optional: for storing Spark data/logs if needed
      - ./spark-logs:/opt/spark/logs # note: sudo chmod 777 spark-logs
      - ./spark-worker:/spark-worker # note: sudo chmod 777 spark-worker